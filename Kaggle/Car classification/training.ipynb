{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * 2019 3rd ML month with KaKR\n",
    "+ 자동차 이미지 데이터셋을 활용한 자동차 차종 분류\n",
    "+ 대회 링크: https://www.kaggle.com/c/2019-3rd-ml-month-with-kakr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/main_img.png\" width=\"70%\" height=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 필요 모듈 임포트 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import ImageOps, ImageFilter, ImageDraw\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, GlobalAveragePooling2D, BatchNormalization, Input\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras import backend as K\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# EfficientNet Download\n",
    "!pip install git+https://github.com/qubvel/efficientnet\n",
    "from efficientnet.keras import EfficientNetB3\n",
    "\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 데이터 확인 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_crop_df(df):\n",
    "    dfcopy = df.copy(deep = True)\n",
    "    dfcopy['img_file'] = dfcopy['img_file'].apply(lambda x : 'crop' + x)\n",
    "    return pd.concat([df, dfcopy], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 및 로드 경로\n",
    "model_path = './model/'\n",
    "if(not os.path.exists(model_path)):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "# 데이터 경로 확인\n",
    "img_path = os.listdir('../input')\n",
    "print(img_path)\n",
    "\n",
    "CROP_PATH = '../input/' + img_path[2]\n",
    "DATA_PATH = '../input/' + img_path[1]\n",
    "\n",
    "# image folder path\n",
    "TRAIN_IMG_PATH = CROP_PATH +'/train_crop/'\n",
    "TEST_IMG_PATH = CROP_PATH +'/test_crop/'\n",
    "\n",
    "# read csv\n",
    "df_train = pd.read_csv(DATA_PATH + '/train.csv')\n",
    "df_test = pd.read_csv(DATA_PATH + '/test.csv')\n",
    "\n",
    "# Centered Crop된 이미지를 사용하기 위해 경로를 crop 폴더로 변경해줌\n",
    "df_train = make_crop_df(df_train)\n",
    "df_train[\"class\"] = df_train[\"class\"].astype('str')\n",
    "\n",
    "df_train = df_train[['img_file', 'class']]\n",
    "df_test = df_test[['img_file']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < EDA >\n",
    "+ 클래스 개수는 특정 유형이 많거나 적은 경우가 1~2개 클래스 존재하지만,\n",
    "+ 실험 결과에 큰 영향을 주지 않으므로 그대로 진행\n",
    "+ 기존 noise가 포함된 이미지보다 crop된 이미지가 성능에 가장 큰 향상을 보여줌으로 이를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 개수 확인\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.countplot('class', data=df_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 이미지 제네레이터 정의 >\n",
    "+ shear_range는 성능에 좋지 않음\n",
    "+ 클래스 불균형으로 인해 class_weight를 추가했지만, 성능 향상을 보이지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# nb_train_samples = len(X_train)\n",
    "# nb_validation_samples = len(X_val)\n",
    "nb_test_samples = len(df_test)\n",
    "batch_size = 32\n",
    "\n",
    "# Define Generator config\n",
    "# https://www.kaggle.com/kozistr/seedlings-densenet-161-48-public-lb-98-236\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range = 60,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    width_shift_range=0.30,\n",
    "    height_shift_range=0.30,\n",
    "    horizontal_flip = True, \n",
    "    vertical_flip = False,\n",
    "    zoom_range=0.25,\n",
    "    fill_mode = 'nearest',\n",
    "    rescale = 1./255)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "def get_steps(num_samples, batch_size):\n",
    "    if (num_samples % batch_size) > 0 :\n",
    "        return (num_samples // batch_size) + 1\n",
    "    else :\n",
    "        return num_samples // batch_size\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(df_train['class']),\n",
    "                                                 df_train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Metric 정의 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Model 정의 >\n",
    "+ 최종 결과에서는 efficientNet, Incep_ResNet, Inception 등 다양한 모델을 학습\n",
    "+ f1 지표가 가장 좋은 모델을 선택하는 것이 최종적으로 좋은 성능을 보여주었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init params\n",
    "lr = 2e-4\n",
    "\n",
    "# 콜백 정의\n",
    "def get_callback(model_path):\n",
    "    callback_list = [\n",
    "              ModelCheckpoint(filepath=model_path, monitor='val_f1_m',\n",
    "                      verbose=1, save_best_only=True, mode = 'max'),\n",
    "              ReduceLROnPlateau(monitor='val_f1_m',\n",
    "                        factor=0.2,\n",
    "                        patience=3,\n",
    "                        min_lr=1e-7,\n",
    "                        cooldown=1,\n",
    "                        verbose=1, mode = 'max'),\n",
    "              EarlyStopping(monitor = 'val_f1_m', patience = 5, mode = 'max')\n",
    "              ]\n",
    "    return callback_list\n",
    "\n",
    "def get_model(base_model, input_size, train_session):\n",
    "    base_model = base_model(weights='imagenet', input_shape=(input_size,input_size,3), include_top=False)\n",
    "\n",
    "    inputs = Input(shape = (input_size, input_size, 3), name = 'input_1')\n",
    "    x = base_model(inputs)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(2048, kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(196, activation = 'softmax')(x)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = x)\n",
    "    if(train_session):\n",
    "        nadam = Adam(lr = lr)\n",
    "        model.compile(optimizer= nadam, loss='categorical_crossentropy', metrics=[categorical_accuracy,\n",
    "                                                                              f1_m, precision_m, recall_m])\n",
    "    return model\n",
    "\n",
    "k_folds = 8\n",
    "img_size = (299, 299)\n",
    "skf = StratifiedKFold(k_folds, random_state = 2019)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    directory=TEST_IMG_PATH,\n",
    "    x_col='img_file',\n",
    "    y_col=None,\n",
    "    target_size= img_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Training >\n",
    "+ 세션이 자주 끊기는 것은 학습에 가장 큰 문제점이었음\n",
    "+ 별다른 방법이 없으므로 끊길 때마다 모니터링해주고, 다시 학습을 진행..\n",
    "+ 간혹 OOM 문제가 뜨는데, gc.collect()는 효과가 없었고, clear_session()이 효과가 있는 것 같았음.\n",
    "+ 단, clear_session()이 for-loop 안에서 동시에 실행되야하고, 다른 셀에서 실행된 경우 효과를 보여주지 않았음(Keras 내부 문제라고..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 1 # Session이 끊기는 경우가 있으므로, 연속된 학습을 위해 설정\n",
    "\n",
    "img_size = (299, 299)\n",
    "model_names = []\n",
    "epochs = 100\n",
    "\n",
    "# mandatory, MODEL name\n",
    "BASE_MODEL = EfficientNetB3\n",
    "\n",
    "for (train_index, valid_index) in skf.split(\n",
    "    df_train['img_file'], \n",
    "    df_train['class']):\n",
    "    \n",
    "    # Session 끊기면 사용\n",
    "#     if(j < 7):\n",
    "#         j += 1\n",
    "#         continue\n",
    "#     elif(j == 8):\n",
    "#         break\n",
    "\n",
    "    traindf = df_train.iloc[train_index, :].reset_index()\n",
    "    validdf = df_train.iloc[valid_index, :].reset_index()\n",
    "    nb_train_samples = len(traindf)\n",
    "    nb_validation_samples = len(validdf)\n",
    "\n",
    "    print(\"=========================================\")\n",
    "    print(\"====== K Fold Validation step => %d/%d =======\" % (j,k_folds))\n",
    "    print(\"=========================================\")\n",
    "\n",
    "    # Make Generator\n",
    "    train_generator_299 = train_datagen.flow_from_dataframe(\n",
    "        dataframe=traindf, \n",
    "        directory=TRAIN_IMG_PATH,\n",
    "        x_col = 'img_file',\n",
    "        y_col = 'class',\n",
    "        target_size = img_size,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    validation_generator_299 = val_datagen.flow_from_dataframe(\n",
    "        dataframe=validdf, \n",
    "        directory=TRAIN_IMG_PATH,\n",
    "        x_col = 'img_file',\n",
    "        y_col = 'class',\n",
    "        target_size = img_size,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    model_name = model_path + str(j) + '_EFF_segcrop.hdf5'\n",
    "    model_names.append(model_name)\n",
    "    new_model = get_model(BASE_MODEL, img_size[0], train_session=True)\n",
    "    \n",
    "    try:\n",
    "        new_model.load_weights(model_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    history = new_model.fit_generator(\n",
    "                        train_generator_299,\n",
    "                        steps_per_epoch = get_steps(nb_train_samples, 32),\n",
    "                        epochs=epochs,\n",
    "                        validation_data = validation_generator_299,\n",
    "                        validation_steps = get_steps(nb_validation_samples, 32),\n",
    "                        callbacks =  get_callback(model_name),\n",
    "                        class_weight = class_weights)\n",
    "        \n",
    "#     j+=1\n",
    "    print(gc.collect()) # 효과 없음\n",
    "    K.clear_session() # 가끔 효과 있음(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Conclusion >\n",
    "+ 다양한 optimizer 실험에서 adam optimizer가 가장 좋은 성능\n",
    "+ cosine annealing 등 실험해보았지만 오히려 학습 시간이 길어져 학습 결과를 확인하는 데 너무 오래걸림\n",
    "+ 1e-4, 2e-4에서 고정된 lr로 시작하는 것이 효과적\n",
    "+ 결과 제출에는 한 가지 모델보다 다른 구조의 모델을 학습하여 앙상블한 경우와 성능 차이가 크게 났음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Prediction >\n",
    "+ 간단한 TTA(Test Time Augmentation)을 사용\n",
    "+ EfficientNet, Xception, Incepres, Cutmix model을 앙상블함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# MODEL folder path\n",
    "EFF_PATH = os.path.join('../input', list_dir[5])\n",
    "XCEPTION_PATH = os.path.join('../input', list_dir[1])\n",
    "INCEPRES_PATH = os.path.join('../input', list_dir[4])\n",
    "CUTMIX_PATH = os.path.join('../input', list_dir[1])\n",
    "\n",
    "xception_model = ['_Xception_f1_8fold.hdf5', Xception]\n",
    "eff_model = ['_EFF_f1_8fold.hdf5', EfficientNetB3]\n",
    "incepres_model = ['_IncepRes_f1_8fold.hdf5', InceptionResNetV2]\n",
    "cutmix_model = ['_cmm.hdf5', InceptionResNetV2]\n",
    "\n",
    "model_list = [xception_model, eff_model, incepres_model]\n",
    "# total predictions list\n",
    "preds_list = []\n",
    "\n",
    "TTA_STEPS = 5\n",
    "\n",
    "for model_name, base_model in model_list:\n",
    "    print(model_name)\n",
    "    # prediction each fold\n",
    "    img_size = 299\n",
    "    predictions = []\n",
    "    fold_num = 8 + 1\n",
    "    \n",
    "    # model_load_dir\n",
    "    if(model_name == '_EFF_f1_8fold.hdf5'):\n",
    "        model_load_dir = EFF_PATH\n",
    "    elif(model_name == '_Xception_f1_8fold.hdf5'):\n",
    "        model_load_dir = XCEPTION_PATH\n",
    "    elif(model_name == '_IncepRes_f1_8fold.hdf5'):\n",
    "        model_load_dir = INCEPRES_PATH\n",
    "    elif(model_name == '_cmm.hdf5'):\n",
    "        model_load_dir = CUTMIX_PATH\n",
    "\n",
    "    for i in range(1, fold_num):\n",
    "        model = get_model(base_model, img_size, train_session = False)\n",
    "        # '..input/EFF_F1_8fold/i_EFF_f1_8fold.hdf5'\n",
    "        model.load_weights(os.path.join(model_load_dir, str(i)) + model_name)\n",
    "        # tta prediction list\n",
    "        tta_preds = []\n",
    "        for _ in range(TTA_STEPS):\n",
    "            if(img_size == 224):\n",
    "                test_generator_224.reset()\n",
    "                pred = model.predict_generator(\n",
    "                generator = test_generator_224, \n",
    "                steps = get_steps(nb_test_samples, batch_size),\n",
    "                verbose = 1\n",
    "                )\n",
    "            else:\n",
    "                test_generator_299.reset()\n",
    "                pred = model.predict_generator(\n",
    "                generator = test_generator_299, \n",
    "                steps = get_steps(nb_test_samples, batch_size),\n",
    "                verbose = 1\n",
    "                )\n",
    "            tta_preds.append(pred) # (TTA_STEP, 6150, 196)\n",
    "        tta_preds = np.mean(tta_preds, axis = 0) # (6150, 196)\n",
    "        predictions.append(tta_preds) # (fold, 6150, 196)\n",
    "        # for memory leaky\n",
    "        del model\n",
    "        for _ in range(10):\n",
    "            gc.collect()\n",
    "        K.clear_session()\n",
    "    preds_list.append(np.mean(predictions, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Make Submission >\n",
    "+ 성능 저하로 인해 cutmix 모델은 제외함\n",
    "+ 가중은 0.5, 0.4 등 다양한 범위의 수치로 조합하여 사용해보았지만, (0.34, 0.34, 0.32)가 제일 좋은 성능을 보여주었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg prediction\n",
    "preds = (preds_list[0] * 0.34) + (preds_list[1] * 0.34) + (preds_list[2] * 0.32)\n",
    "preds_class_indices=np.argmax(preds, axis=1)\n",
    "preds_labels = (train_generator_299.class_indices)\n",
    "labels = dict((v,k) for k,v in preds_labels.items())\n",
    "final_predictions = [labels[k] for k in preds_class_indices]\n",
    "\n",
    "# make submission file\n",
    "submission = pd.read_csv(os.path.join(CSV_PATH, 'sample_submission.csv'))\n",
    "submission[\"class\"] = final_predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
